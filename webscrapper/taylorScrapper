import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime

def scrape_google_news(query):
    # URL for Google News search
    url = f"https://news.google.com/search?q=taylor%20swift%20when%3A1d&hl=en-US&gl=US&ceid=US%3Aen"
    
    # Send a request to the URL
    response = requests.get(url)
    
    # Parse the HTML content of the page
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find all news items
    news_items = soup.find_all('div', class_='xrnccd')

    articles = []
    for item in news_items[:25]:  # Limit to first 25 articles
        title = item.find('h3', class_='ipQwMb ekueJc RD0gLb').text
        link = "https://news.google.com" + item.find('a')['href'][1:]
        content = fetch_article_content(link)

        articles.append({'link': link, 'title': title, 'content': content})
    
    return articles

def fetch_article_content(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        content = soup.find('p').get_text()  # Example, adjust as needed
        return content
    except Exception as e:
        return "Error fetching content: " + str(e)

def save_to_csv(articles, filename):
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(['URL', 'Title', 'Content'])
        for article in articles:
            writer.writerow([article['link'], article['title'], article['content']])

# Main execution
taylor_swift_news = scrape_google_news("Taylor Swift")
save_to_csv(taylor_swift_news, 'taylor_swift_news.csv')